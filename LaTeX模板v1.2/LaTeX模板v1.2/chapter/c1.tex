\chapter{引言}


\section{项目背景}

随着信息技术的发展，人类采集、分析数据的能力越来越强，逐渐迈进大数据时代。大数据时代不仅带来了数据量的增长，也带来了数据类型的多样化。随着5G时代的临近，智能终端、无线传感器组成的数据网络会带来更复杂的数据类型，给数据存储和分析带来巨大压力。图数据是一种由点边组成的半结构化数据，用于映射事物之间的关系，如人际关系、交易往来、交通道路等模型，具有极强的现实意义。如图~\ref{graph-model}，属性图（Property Graph）是近年来兴起的一种图模型，在点、边上可以自由定义属性和类型，从而形成社交网络、交易网络等复杂图。

\begin{figure}[htb]
    \centering
    \includegraphics[width=5in]{FIGs/c1/graph-model.png}
    \caption[属性图模型示例]{属性图模型示例~\cite{StellarDBMainPage}}\label{graph-model}
  \end{figure}

传统关系型数据库擅长处理拥有固定结构的表格型数据，通过一些JOIN操作来得到数据之间的关联关系。在数据量增长或数据类型复杂时 ，关系型数据库会存在几个瓶颈。其一，为了获得数据之间的连接信息，关系型数据库不得不通过JOIN的方法来取得“下一跳”节点。大量的JOIN操作不仅对计算资源造成极大浪费，也无法快速返回数据结果。其二，图数据在应用场景中可能频繁地修改数据模型，关系型数据库在应对这种场景时，对用户的模型设计能力要求极高。关系型数据库由于数据模型限制而无法适配图场景，图数据库因此孕育而生。相较于关系型数据库，图数据库在这些方面具有优势：拥有灵活可变的数据结构；充分利用图的内联信息，可存储规模庞大的关系；实时返回查询结果。

如图~\ref{DB-trends}，根据著名数据库统计网站DB-Engines的统计，图数据库在短短数年内获得了远超其他类型数据库的关注~\cite{DB-popularity}。图数据库把边视作数据的一种，关系型数据库的JOIN操作转换为图数据库的一次普通查询。在数据量增加时，JOIN操作会急剧增加查询的开销，但对于图数据库仅会增加少量开销。随着图数据数量增加，单机系统在计算和存储上存在明显瓶颈，分布式图数据库是未来的趋势。

\begin{figure}[htb]
    \centering
    \includegraphics[width=5in]{FIGs/c1/DB-trends.png}
    \caption[图数据库流行度趋势上升迅速]{图数据库流行度趋势上升迅速~\cite{DB-popularity}}\label{DB-trends}
  \end{figure}

Transwarp StellarDB是星环公司为企业级图应用打造的一款分布式图数据库，用于快速查找数据间的关联关系，并提供强大的算法分析能力。
StellarDB克服了海量关联图数据存储的难题，通过自定义图存储格式和集群化存储，实现了传统数据库无法提供的低延时多层关系查询，在社交网络、公安、金融领域都有巨大应用潜力。结合原生存储引擎和计
算引擎，StellarDB可以轻松实现数千亿边规模的海量图存储，实时数据插入更新，10层以上深度链路查询，以及复杂算法分析~\cite{StellarDBMainPage}。

StellarDB的主要优势包括原生图存储、灵活数据模型、快速数据导入、存储计算融合和实时更新与查询。StellarDB为数据存储设计了专有的图存储结构，并通过高效的压缩算法减少磁盘和内存的使用量。根据分区策略，图数据被分散于集群各节点，拥有良好的可扩展性。此外，StellarDB通过星环分布式存储引擎Shiva为每份图数据创建多个副本，以保证数据的容错性、一致性和高可用性。StellarDB允许用户构建类型丰富的属性图，可以给图中不同的边、点实体添加不同的标签，每个标签下的数据有独立的属性和索引，允许对数据模型和索引字段进行修改和更新。StellarDB拥有强大的数据导入功能，通过交互式界面可以快速配置和导入任务，同时也支持调用Java API插入数据，导入速度可达60GB/小时。StellarDB将存储引擎和计算引擎结合，使计算引擎可以利用数据locality提升计算性能。StellarDB支持数据的实时更新和查询，支持按照边点主键或者属性条件批量更新或批量删除。

StellarDB在很多行业都有广泛的应用。
StellarDB可以存储客户购买历史记录，好友列表和感兴趣商品等关联信息，支持多层次商品推荐的计算方式。通过构建商品与兴趣标签的知识图谱，可构建客户的兴趣画像并关联商品；或通过关联具有相似购买记录的人群，计算人物相似度并关联相似人群的高评分商品。
通过关联查询、可视化图分析、图挖掘、机器学习和规则引擎的结合使用，可快速检索关联关系数据，并挖掘隐藏关系并模型化业务经验，帮助金融机构的建立一个可持续、经济可行的反洗钱合规框架。
通过筛选分析合作关系、集团关系、投资关系、社团分类关系以及资产与负债等业务数据，来识别风险客户和风险集团，降低人力成本消耗并大幅提升反欺诈能力。

\section{图数据库发展现状}

\subsection{主流图数据库}

如图~\ref{graph-db-trend}，目前的主流图数据库都是国外公司的产品。有些是开源/商业数据库产品，如Neo4j, Janus Graph；有些则是大公司开发自用，如Facebook的RocksDB、Google的LevelDB。

\begin{figure}[htb]
  \centering
  \includegraphics[width=5in]{FIGs/c1/graph-db-trend.png}
  \caption[主流图数据库流行度趋势]{主流图数据库流行度趋势~\cite{graph-db-popularity}}\label{graph-db-trend}
\end{figure}

Neo4j由Neo4j公司开发，是一个嵌入式的、基于磁盘的、具备完全的事务特性的Java持久化引擎~\cite{DBLP:conf/btw/Wiese19}。它提供同时提供开源社区免费版和商业版，是“DB-Engines Ranking”上流行度排名第一的图数据库，在所有数据库中流行度排名为第22位~\cite{DB-popularity}。

Neo4j使用Java开发，而且支持通过Cypher查询语言在其他编程语言进行调用。对Neo4j的操作既可以使用HTTP协议进行请求，也可以使用Neo4j自创的二进制的Bolt协议调用~\cite{Bolt}。

作为流行度最高的图数据库，Neo4j各方面示例都强大、均衡：Neo4j性能强大，可以在对数据量大、连通度高的图数据进行多跳查询时保障稳定、即时返回的性能。Neo4j基于Raft技术实现自管理群功能，能够滚动更新、提供实时切换的热备份，保障了高可用性。Neo4j的属性图模型使对图数据的构建、查询易于实施，提供了使用上的灵活性。Neo4j实现了细粒度的安全管理，包括LDAP/目录服务，安全登录等~\cite{Neo4jMainpage}。



JanusGraph是一款Linux基金会管理下的开源分布式图数据库。其开源协议是Apache License 2.0~\cite{JanusMainpage}。

JanusGraph的开发背后还有有诸多公司的支持，如IBM和Google~\cite{JanusSupport}。所以JanusGraph在与其他项目的协作上灵活度很高。用户可以选择多种存储引擎作为JanusGraph的底层支持，包括Apache基金会的Cassandra、HBase，Google公司的云端BigTable，Oracle的BerkeleyDB，Scylla等等。

JanusGraph尤其与Apache基金会的各种大数据项目协作紧密：JanusGraph的支持图数据全局分析报告，可以与其他大数据平台，如Apache Spark、Apache Giraph、Apache Hadoop等进行ETL集成。JanusGraph支持外部索引存储，如Elastic~Search，Apache Solr和Apache Lucene等。通过外部的索引存储，JanusGraph可以对地理坐标、数值范围、文本类型等数据进行搜索匹配。JanusGraph与Apache TinkerPop的图技术栈有着底层集成，包括Gremlin图查询语言、Gramlin图服务器和Gremlin应用。


TigerGraph是一款高性能、高伸缩性的图数据库，通过对图数据结构的底层实现，它可以为了容量或速度的不同考量进行轻松缩放。TigerGraph通过其独特的“Native Parallel Graph”技术，可以支持实时大规模数据即的完全分布式并行分析。

TigerGraph的关键优势包括其查询语言GSQL，以及其对大数据集的支持~\cite{DBLP:journals/corr/abs-1901-08248}。TigerGraph所使用的查询语言GSQL不同于Neo4j的Cypher，更加相似于传统SQL语言，所以对SQL用户更友好、更易学。TigerGraph在大数据集支持方面，导入速度可以达到单机每小时导入50GB至150GB，而对点、边的遍历查询速度可达单机每秒上亿条~\cite{TigerGraph}。


\subsection{图数据库存储特点}

图数据库的概念只是限定了它给用户呈现的数据模型抽象应当是图，而对于内部存储的实现没有限制。所以图数据库的存储可以使用传统的表来实现；也可以使用键值对存储，强调NoSQL优势~\cite{DBLP:conf/icdt/Reutter20}。StellarDB所采用的LSM树就属于一种键值对存储，这也是主流图数据库的共同选择。因为LSM树能够很好地满足大多数图场景对频繁写的需求：不论是新增、更新还是删除操作，时间复杂度与数据量都是线性关系。因为compaction操作（见2.1节）与客户的写入是异步的，而且只要compaction性能不过差，就基本不会影响到客户的读写请求。

根据需求不同，图数据库还可以在磁盘读写为主和全内存缓存之间进行选择。StellarDB由于要应对众多客户的复杂场景，选择了依靠磁盘读写与内存的部分缓存。而一些目标场景明确、硬件资源充足的图数据库，如领英自用的GraphDB，就采用了数据内存全缓存的方案，在性能上明显高于依赖磁盘的数据库~\cite{GraphDB}。


\section{论文主要工作}

本文首先介绍了StellarDB产品背景，说明了其主要特点与优势，然后介绍了StellarDB的存储模块的设计，解释了它对LSM树数据结构的实现与维护方法。

本文描述了StellarDB在“大数据集高并发写入”之后的性能问题：与处于空闲状态下不同，此时数据库系统无法在预期时间内响应客户端的查询请求，导致客户端请求超时失败。

之后本文对性能问题进行了分析：性能数据显示，StellarDB在对于LSM树数据结构进行compaction处理时，第0层的数据向下流动速度极慢，导致L0数据文件数目严重膨胀。在LSM树数据结构下，读数据需要搜索每一个L0数据文件。这就导致了在写入频繁的应用场景中StellarDB容易超负荷、读写操作响应超时的缺陷。

本文描述了上述性能问题的分析与优化的设计与实现。由于磁盘读写速度已达瓶颈，所以合理的优化方式是减少compaction过程中的读写浪费，也就是在第0层的数据与第1层的数据合并时，降低第1层的数据所占比重。本文实现了对于flush算法与compaction算法的联合优化：首先，在flush算法中加入“内存缓冲区强制分片”，提供“单个L0文件的数据主键范围较小”的条件。然后，在compaction算法中通过“文件选取顺序倒置”与更精细的文件选择算法，将I/O浪费率降至最低。这样的优化能够减少对同等数据进行compaction所消耗的磁盘I/O量。

最后，本文展示了StellarDB原测试数据与新的优化模拟程序的测试结果，证明了算法的有效性：L0至L1的compaction读扩大率从419\%下降至119\%，数据在LSM树中不再堆积于上层。经过这样的优化，StellarDB的存储模块提高了compaction效率，消除了读请求超时失败的性能问题，获得了在写入频繁的场景中读写性能的全面提升。

\section{论文组织结构}

本文的组织结构如下：

第一章，引言部分。介绍了StellarDB的项目背景及图数据库国内外研究现状，简要的介绍了本文的主要工作。

第二章，技术综述。介绍了StellarDB系统实现过程中所涉及的技术，包括LSM树数据结构、Raft一致性协议等技术的相关介绍。

第三章，StelllarDB存储模块的分析与设计。首先，分析了该模块的地位、与其他模块的协作关系。然后，大致说明本模块内的组件协作与划分，详细介绍模块对LSM树数据结构的维护方式。最后，简要介绍优化模拟测试程序的设计目标。

第四章，StellarDB存储模块局部及其模拟程序的详细设计与实现。其中通过源代码等方式详细说明了存储模块的flush、compaction部分的设计与实现细节，重点展示在写入处理方面的优化；展示了优化模拟程序的详细设计。性能测试部分展现了在高性能集群下进行使用实际数据的严格控制变量的StellarDB写入性能对比测试的结果，以及优化模拟程序的测试结果。

第五章，总结与展望。总结本论文所做的工作，并对StellarDB的未来发展方向做进一步的展望。

