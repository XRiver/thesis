\chapter{技术综述}

本章介绍StellarDB存储引擎系统在设计与开发过程中用到的相关理论和技术。

\section{Log-Structured Merge-Tree数据结构}

日志结构合并树（Log-Structured Merge-tree，下文称LSM树）是基于磁盘的数据结构，旨在在长期运行过程中在文件上提供记录的高频率插入和删除，以及低成本索引~\cite{DBLP:journals/acta/ONeilCGO96}。其设计目的在于获得比传统的B+树与索引顺序存取方法更好的写入速度。LSM树使用延迟和批量索引更改的算法，将更改从内存中的组件通过一个或多个磁盘组件进行级联~\cite{DBLP:conf/socc/MittalN19}。这种高效的方式类似于到合并排序。在此过程中，所有索引值都可以通过内存组件或磁盘组件连续访问。相比B树等传统访问方法，该算法大大减少了磁盘臂的移动~\cite{DBLP:conf/apweb/ZhuHQZLZ17}。LSM树还可以推广到其他操作，比如插入和删除~\cite{DBLP:conf/IEEEcloud/SongYH18}。

LSM树的基本设计比较简明。各批写入操作会会被按顺序保存到一组较小的索引文件中，每个文件都包含涵盖一段时间内的变更。文件按照写入时间排序以保证之后能快速搜索。文件本身完成写入之后就不会被更新，对数据的更新总是被写到新创建的文件中去。读操作会检查所有的数据文件。数据文件需要被周期性地合并，来保证其数量不过度膨胀。将数据从内存组件向磁盘的流动操作称为flush，将数据在磁盘中多个逻辑分层的数据文件之间的流动称为compaction。这两种操作是维护LSM树的主要操作。

当处理数据更新操作时，系统首先应将其加入内存缓冲区，并维持缓冲区的数据主键顺序。通常，内存缓冲区会有预写式日志（write-ahead-log）支持，以保障系统可以恢复写入内存缓冲区的数据。当内存缓冲区被数据填满，就会通过flush操作写到磁盘上的一个新文件中。随着数据的不断写入，flush操作也不断重复。如图~\ref{LSM-1}，新写入的数据条目只会创建新的顺序的不可变的数据文件，而非修改包含此主键条目的老数据文件。

\begin{figure}[htb]
    \centering
    \includegraphics[width=5in]{FIGs/c2/LSM-1.png}
    \caption{LSM树通过flush操作增加数据文件}\label{LSM-1}
\end{figure}

当需要从LSM树种读取数据条目时，系统首先需要搜索内存缓冲区，然后以从旧到新的时序搜索各个数据文件，然后将搜索到的条目按时序合并，得到此条目的当前状态。显然，数据文件的个数会影响读取时间，所以需要减少数据文件的数目。

系统周期性地对LSM树进行compaction操作。Compaction操作会选择几个数据文件，把合并到一起输出，并删除原来的文件。其中的具有重复主键的条目会被合并成一条。这个操作的意义不仅在于合并条目，更重要的是减少数据文件的数目，避免数据文件数目过多导致读取性能下降。由于数据文件中的数据条目都是按照主键排序的，所以compaction操作相当高效，就像合并排序，对每个输入文件只需要遍历一遍即可~\cite{LSMT-blog}。为了使LSM树的数据文件更易管理，LSM树的具体实现可以给数据文件标注逻辑上的层次。如图~\ref{LSM-2}，层数标识数据文件的新旧，刚刚写入的数据文件都处于第0层（Level 0，简称L0），也就是最高层。除了L0以外，每层的数据文件都可以看作一个整体，层内各文件保持有序，数据主键不重叠，就像是一个巨大的数据文件被切分不同区域。通过规定compaction只能从高层向低层写（或最底层文件之间互相合并），旧的数据文件之间的时间顺序与主键顺序更加清晰。这样做还就控制了读数据时需要搜索的数据文件个数：少于L0文件数与分层数的和。

\begin{figure}[htb]
    \centering
    \includegraphics[width=5in]{FIGs/c2/LSM-2.png}
    \caption{分层compaction示意图}\label{LSM-2}
\end{figure}


包括Google Bigtable、Apache Cassandra等数据库都采用了LSM树作为数据存储结构~\cite{CompactionInCass}。
StellarDB也采用LSM树作为数据记录的存储结构。StellarDB目前没有支持使用多种磁盘组件的级联，数据仅在逻辑上有额外的层次划分。

\section{Raft一致性协议}

为了保证多副本数据最终状态一致，StellarDB使用Raft一致性协议作为多副本的协调机制，并利用Raft的日志（log）来短期备份写入的数据。

\subsection{Raft概念}

Raft是由Diego Ongaro与John Ousterhout的一篇论文~\cite{DBLP:conf/usenix/OngaroO14}中所提出的分布式存储一致性算法。在分布式系统中，为了防止服务器数据由于只存一份而导致在服务时由于一个存储节点故障就产生服务完全不可用或数据丢失的严重后果，数据的存储会有多个备份副本，分别存储于不同的存储服务器上，都可以提供服务。这样一来，如果有合适的算法能保障各服务器对同一份数据存储的内容一致，并且在一台服务的存储服务器故障时，这个集群能以适当的逻辑切换到其他正常服务器提供服务，那么就可以实实在在地保障分布式存储服务的质量。Raft就是为这样的系统服务的。

\subsection{Raft的特点}

简单易学。Paxos算法由Leslie Lamport发表于1990年，是当时最实用的分布式存储一致性算法~\cite{DBLP:journals/tocs/Lamport98}。而Raft是由Paxos简化得来。Diego Ongaro与John Ousterhout指出：经过对比，学生学习Paxos所需时间明显长于学习Raft所需时间~\cite{DBLP:conf/usenix/OngaroO14}。

最终一致性。存储一致性根据对于各个服务器的同一份数据之间允许差异的严格程度不同，可以分为强一致性、最终一致性、弱一致性。根据“CAP定理”，一个分布式系统不能同时保障一致性（Consistency）、可用性（Availability）与分区容错性（Partition tolerence）。但是经过权衡，系统可以达到“BASE”效果：基本可用（Basically available）、软状态（Soft state）、最终一致性（Eventually consistent）。Raft所达到的最终一致性，是指各节点上的数据在经过足够的时间之后，最终会达到一致的状态~\cite{DBLP:journals/cj/Munoz-EscoiJGMB19}。

\subsection{Raft的选举}

Raft集群各机之间的RPC报文可分为两种：添加条目RPC（AppendEntries RPC，以下简称AE）与请求投票RPC（RequestVote RPC，以下简称RV）。AE是~leader用来向follower加entry使用的。RV是“候选人"（candidate，是除了leader、follower以外的第三种状态，只出现于选举时）用来向其他follower要求给自己投票的。

如果超过一定时间，follower检测不到来自leader的周期性心跳消息（leader将不含实际entry的AE作为心跳使用），就会变为candidate状态。此时，Raft集群就会开始选举leader。在Raft中，时间上有着term的概念，表示一个leader的统治期；每个term有着独特的递增的序号，称为term ID。leader会在自己发送的AE中都附上自己的term ID。当新的candidate产生，它会在上一个leader的term ID基础上加1，作为自己的term ID，并在广播给所有其他机的RV中也附上新的term ID。任何非candidate的节点收到了带有比自己已经见过的任何ID更大的term ID的RV时，就会回复这个RV，并更新“自己已经见过的最大ID”。如果同时这个RV中说明的candidate含有的日志足够新（详细说明见下一小节），follower就会为这个candidate投票。这样一来，任何节点就都不会为同一个term ID投两票。如果一个candidate收到了足够的票数（票数加上自己的一票能够占集群的多数），就会开始发送心跳，宣告自己在这个term内的leader地位，开始服务。

由于在一个leader失效时，可能有多个follower超时的时刻相同，发出RV广播的时刻也大致相同，结果在新term都得不到足够的票数，所以candidate存在等票超时与随机等待机制，避免一致冲突，选不出leader。candidate在等足够的票时当然也会看有没有其他candidate宣布胜利。如果都没有发生的话，在一段超时后，candidate们会再开启新term ID，但会随机等待一段时间，然后才广播RV请求投票（广播RV前相当于follower，可以投票）。由于随机等待的时间有长有短，最终一定会由率先结束等待的candidate获胜。

\subsection{Raft的日志复制}

前面提到，Raft集群由leader统一处理所有客户端请求，会将写请求转换为log entry，然后用AE向follower发送来复制log。Leader创建日志条目时，会给它附上两个属性：term ID与log index。term ID即为自己统治期的ID，在当前term的日志条目都用这个ID；而log index也是一种递增序号，但它是在集群的整个运行期间连续的。跨term时，log index会在前面的的基础上递增1，而非归零重计。如此一来，考虑到选举机制保证了一个term ID一定对应确定的一个leader节点，我们由term ID+log index这个组合就一定可以确定唯一的一条日志。

Leader生成了写操作的日志之后，就通过AE将日志条目发送到各个follower~处，让它们把日志按早晚顺序加入自己的日志存储中。如果有集群多数的节点（包含leader自己）都成功存储了一条日志（follower会回复自己的存储情况），那么leader就认为这条日志及更早的日志的存储都是安全的，会回复客户端写操作成功，并会告知集群已经可以将到此条的所有日志中的写操作实际进行，修改自己的存储数据。

当leader上台时，会以自己自己的日志存储为准，使其他节点与自己对齐。如果比自己快，就截掉多的；比自己慢，就用自己的日志存储给它慢慢补足。但是在复制日志的过程中，各个follower可能因各种原因速度差异较大。那么如果leader突然故障，而一个复制的特别慢的follower选举为leader，就可能会导致大量写操作失效。之所以要保证日志已经复制到了多数机上，才可认为写操作成功，就是为了避免这种情况。之前在讲解选举机制时提到：RV要包含candidate的日志存储版本消息，也就就是最后一条日志的term ID+log index。如果投票的follower发现这candidate的版本消息比自己的还要旧，就会拒绝给它投票。由于只有复制到了多数节点的日志的写操作才被回报为“成功”，而选举时必须要获得多数票才能当选，所以最终当选的leader一定拥有上一个leader所回报为成功的操作的所有日志，不会缺失。

\section{Zookeeper}
StellarDB客户端使用Zookeeper对服务器端进行感知，通过与服务器端共享Zookeeper集群获取集群IP等关键信息。ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务。Zookeeper 能够保证其数据的可靠性和高可用性，并且提供了多种编程语言的 API。

Zookeeper 由多台服务器组成，每台服务器中都复制了一份元数据，程序通过访问任意一台服务器都可以获得完整且一致的数据。通过这种方式，Zookeeper 实现了高可用性、高可靠性和高性能~\cite{DBLP:conf/usenix/HuntKJR10}。Zookeeper 服务器集群中的多台服务器分为两种角色：其一是 leader（主节点），正常情况下，只有一台服务器作为 leader；leader 是由所有服务器自主选举产生的~\cite{DBLP:conf/podc/JunqueiraR09}。当有写入数据操作时，由 leader 负责将写入操作传播至其他服务器，并由 leader 返回写入结果。其他服务器的角色是 follower，读数据的操作可以由 follower 直接处理。leader 接收来自 follower 的写入数据操作，并将结果返回 follower~\cite{DBLP:conf/srds/HalalaiSRF14}。

Zookeeper 中，使用一种结构类似于文件存储系统的树状层次模型来存储数据。在该模型中，节点被称为 Znode，每个节点都有其唯一的标志路径。Znode 既可以直接存储数据，又可以在其中新建 Znode 子节点。Znode 是有版本的，每个节点中可以存储多个版本的数据。Znode 可以被其他程序注册监听，当该节点发生变化（更新，删除）时，Zookeeper 将会自动通知监听该节点的程序~\cite{DBLP:conf/icac/FrommgenHPK17}。

\section{HDFS}

HDFS（Hadoop Distributed File）是Apache Hadoop自带的一个开源的分布式文件系统。HDFS通过多机备份机制实现了高容错以及高并发的特性，同时由于 HDFS 对部署机器要求较低，适合于部署在大量通用的PC机上，因此HDFS被广泛应用于海量数据处理过程中的数据存储~\cite{Hadoop}。StellarDB支持从HDFS解析导入图数据。

HDFS 系统是一个典型的主从结构集群，集群中的节点分为两种，一个namenode和多个datanode。Namenode是中心服务器，负责管理 HDFS 文件系统的命名空间以及客户端对文件的访问，保存文件系统树、访问控制信息、文件/数据块映射关系等文件元数据。Datanode 负责保存数据块，同时需要响应文件系统客户端的读写请求~\cite{DBLP:books/daglib/0035689}。

与磁盘的存储方式类似，在HDFS中，文件也是以数据块为单位进行存储的。文件存储到 HDFS 时会被分割成数据块大小的多个分开，如果文件大小小于 HDFS 系统设定的数据块大小，文件并不会占据整个数据块空间，而是会与其他文件共同占据该数据块。由于过大的文件会被分割成较小的数据块，数据块可以存储于 HDFS 系统中的任意磁盘中，所以 HDFS 系统中可以存储超大的文件，而不需要受到磁盘大小的限制。

由于数据块的大小是固定的，所以极大的简化了 HDFS 系统的设计复杂度。而且文件的权限信息等元数据得以与文件数据分开管理。与此同时，HDFS系统通过将每个数据块在多台机器上进行备份，提高了数据访问的并发度以及文件系统的容错能力，进而降低了对于部署机器的硬件水平的要求，使得 HDFS可以部署在普通的 PC 机集群上而不会发生数据丢失。

当需要读数据时，客户端首先要向namenode发送读数据请求；namenode根据客户端提供的文件信息，判断文件是否存在以及客户端是否具有相应文件
的读取权限，并返回文件地址或拒绝访问响应；如果客户端收到文件地址，会根据文件地址向对应的 datanode 直接发送读数据请求；datanode 接收到客户端
的请求后会返回数据。

当需要写数据时，客户端要向namenode发送写数据请求；namenode判断是否允许写入，并向客户端返回判断结果；如果客户端收到允许写入响应，会将文件进行分块，并向namenode申请数据块空间namenode将返回数据要写入的 datanode 列表；最后，客户端将数据写入datanode。

\section{Docker}

由于基于虚拟机（Virtual Machine）进行的传统应用部署方式所占用的资源较多，操作步骤较冗杂繁多，且启动速度非常缓慢，无法很好地适应当前业务流量的高度弹性化以及微服务架构的技术环境。另外一种虚拟化技术DD容器（Container）技术应运而生，容器技术因为其启动速度快、资源占用少、体积小的诸多优点迅速得到主流软件开发业界的追捧。

Docker是一个开源的轻量级应用容器引擎，基于Go语言实现，并且遵从Apache 2.0开源协议~\cite{Docker}。Docker是目前业界最为流行的容器解决方案，可以
看做是对容器的一种封装，为开发者提供了方便易用的容器使用接口。

Docker能够帮助开发者对于应用以及依赖打包到一个轻量级、可移植的容器镜像（Docker Image）中~\cite{DockerTutorial}。打包完成之后，一方面可以将打包后的容器
镜像发布到Windows或者主流的Linux机器上；另外一方面也可以实现虚拟化。Docker容器完全基于沙箱机制，相互之间不存在任何接口。除此之外，相比于
虚拟机，容器性能开销也非常低。Docker的接口非常简单，因此开发者可以极为方便地创建和使用容器，以及将应用程序放入Docker容器中，并对Docker容
器进行版本管理以及修改。

在云计算时代，应用的弹性伸缩能力至关重要。而Docker容器可以随开随关，因此非常适合用于弹性扩容和缩容。此外，对于微服务架构而言，借
助Docker等容器技术，可以使多个服务实例在同一台物理机上运行，能够有效地降低资源消耗。此外，Docker使得开发者能够将应用程序和基础架构进行分
离，并使用与管理应用程序相同的方式来管理基础架构，从而实现应用程序的快速交付和快速部署，大大减少应用程序从开发环境的编写到在生产环境中实
际运行之间的延迟。StellarDB的部署方式便是以Docker容器的形式运行于服务器。

\section{Kubernetes}

Kubernetes是Google于2014年启动的自动化容器操作的项目，由开源社区维护。 此技术旨在解决集群中容器的部署、 更新、 维护等一系列问题。Kubernetes源于Google内部的Brog容器管理系统，其核心就是将分布在不同主机的应用容器，通过网络组件连接成一个网络，从而实现服务的分布式计算。StellarDB部署采用Kubenetes管理Docker容器，以利用其对容器的自动创建、故障重启、网络共享等功能降低运维难度。


Kubernetes包括以下几个重要的概念：Cluster、Master、 Node、 Pod、 Service。 一个Kubernetes 系统通常称为一个集群，集群主要包括一个Master和若干Node。 Master节点包括API Server、 Scheduler、Controller Manager、etcd，主要负责管理和控制。其中API Server是整个系统的对外接口，可供客户端和其他组件调用；Scheduler负责对集群内部的资源进行调度；Controller Manager 负责管理控制器。Node节点包括Docker、Kubelet、Kube-proxy、Fluentd、Kube-dns以及Pod。Service可以看作一组提供相同服务的Pod 的对外访问接口。Docker用于创建容器；Kubelet主要负责监视指派到它所在Node上的Pod，包括创建、修改、监控、删除等操作；Kube-proxy主要负责为Pod对象提供代理；Fluentd主要负责日志收集、存储与查询~\cite{K8S}。

\section{本章小结}

本章介绍了StellarDB存储模块主要使用的几种技术：用于保存数据、提高写入性能的LSM树数据结构，用于保障多副本数据一致性的Raft一致性协议，用于集群间同步与服务器探查的Zookeeper，基础的分布式存储HDFS，用于StellarDB部署的Docker和Kubernetes。对于LSM树的详细介绍会与系统设计一起包含于下一章。